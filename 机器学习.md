# Three axes of ML
- Data
	- Fully observed 
	- Partially observed
		- Some variables systematically not observed(topic of a document)
		- Some variables missing some of the time(missing data)
		- Actively collect/sense data
- Tasks(What is the type of knowledge that we seek from data)
	- Prediction tasks-Estimate output given input
		- Classification
		- Regression
	- Description tasks
		- Typically synonymous with unsupervised learning
		- Density estimation, Clustering, Dimensionality reduction
- Algorithms
	- Typically use Probabilistic Model of the data

	![[Pasted image 20231230232617.png]]



1. Understand the application of frequentism and Bayesian methods in ML;
2. Understand supervised learning and unsupervised learning;
3. Understand the basic methods of data analysis in ML and their applications
4. Understand maximum likelihood estimation
5. Understand maximum posterior probability estimation
6. Understand the basic concepts of hypothesis testing
7. Understand classification and regression tasks in supervised learning
8. Understand the principles of clustering algorithms in unsupervised learning
9. Understand the basic concepts of deep learning

Neural Network
Linear Regression


# MLP多层感知机
多层感知机MLP(Multilayer Perceptron)，也叫人工神经网络ANN(Artificial Neural Network)，除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构，如下图：
![[Pasted image 20240101230430.png]]
![[Pasted image 20240101230638.png]]
从上图可以看到，多层感知机层与层之间是全连接的。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。
隐藏层的神经元怎么得来？首先它与输入层是全连接的，假设输入层用向量X表示，则隐藏层的输出就是$f (W_1X+b_1)$，$W_1$是权重（也叫连接系数），$b_1$是偏置，函数f可以是常用的sigmoid函数或者tanh函数：

## sigmod函数
$$\sigma(x)={{1}\over{1+e^{-x}}}$$
导数为$$\sigma'(x)=\sigma(x)(1-\sigma(x))$$
sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。

sigmoid缺点：
激活函数计算量大，反向传播求误差梯度时，求导涉及除法
反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练
Sigmoids函数饱和且kil训l掉梯度。
Sigmoids函数收敛缓慢。

## Tanh函数
$$tanhx={{sinhx}\over{coshx}}={{e^x-e^{-x}}\over{e^x+e^{-x}}}$$
导数
$$(tanhx)'=seh^2x=1-tanh^2x$$

取值范围为[-1,1]，tanh在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果。与sigmod的区别是 tanh 是0的均值，因此在实际应用中tanh会比sigmod更好。在具体应用中，tanh函数相比于Sigmoid函数往往更具有优越性，这主要是因为Sigmoid函数在输入处于[-1,1]之间时，函数值变化敏感，一旦接近或者超出区间就失去敏感性，处于饱和状态。

最后就是输出层，输出层与隐藏层是什么关系？

其实隐藏层到输出层可以看成是一个多类别的逻辑回归，也即softmax回归，所以输出层的输出就是$softmax(W_2X_1+b_2)$，X1表示隐藏层的输出$f(W_1X+b_1)$。

MLP整个模型就是这样子的，上面说的这个三层的MLP用公式总结起来就是，函数G是softmax。

因此，MLP所有的参数就是各个层之间的连接权重以及偏置，包括W1、b1、W2、b2。对于一个具体的问题，怎么确定这些参数？求解最佳的参数是一个最优化问题，解决最优化问题，最简单的就是梯度下降法了（SGD）：首先随机初始化所有参数，然后迭代地训练，不断地计算梯度和更新参数，直到满足某个条件为止（比如误差足够小、迭代次数足够多时）。这个过程涉及到代价函数、规则化（Regularization）、学习速率（learning rate）、梯度计算等，本文不详细讨论，读者可以参考本文底部给出的两个链接。
![[Pasted image 20240102112431.png]]

# Fully Connected Network全连接网络
## 单层神经网络
Logistic回归模型是最简单的单层网络，常被用来处理二分类问题，它使一种用于分析各种影响因素$x_1, x_2, ..., x_n$）与分类结果y之间的有监督学习方法。
![[Pasted image 20240102103633.png]]
### 正向传播
此计算过程等同于线性回归计算，即给每一个输入向量x分配权值，计算出一个结果向量z。同时，为了使神经网络具有非线性特点，引入激活函数来处理线性变换得到的数值。
线性变换（加权和偏置）：$z=w^Tx+b$
·非线性变换（激活函数）：$δ(x)=1+{{1}\over{1+e{-z}}}$
上式中w为权值，b为偏置，x为输入值，z为线性输出值，δ为非线性输出值。
### 损失函数
模型需要定义损失函数来对参数w和b进行优化，损失函数的选择需要具体问题具体分析，以下为两种常见损失函数计算公式。
- 平方损失函数：$L(\hat{y}，y)={{1}\over{2}}(\hat{y}-y)^2$
- 对数似然损失函数：$L(\hat{y}，y)=-[y1og\hat{y}+(1-y)1og(1-\hat{y})]$
上式中$\hat{y}$为计算结果，y为实际结果
![[Pasted image 20240102113212.png]]
### 梯度下降
梯度下降是一种前反馈计算方法，反映的是一种“以误差来修正误差”的思想，亦是神经网络进行迭代更新的核心过程。
- 迭代更新：
$$\omega=\omega-\alpha{{dL(\omega)}\over{d\omega}}$$
- 链式法则：
$${{dL(a, y)}\over{d\omega}}={{dL(a, y)}\over{da}}{{da}\over{dz}}{{dz}\over{d\omega}}$$
 
### Why deep not fat
![[Pasted image 20240102111820.png]]

## 浅层神经网络
浅层神经网络相比单层网络的差别在于隐藏层有多个神经节点，这就使得其可以处理“多输入多输出”的复杂问题。每一层的每一个节点都与上下层节点全部连接，这种神经网络称作全连接网络。![[Pasted image 20240102113514.png]]
### 正向传播
$$\left.\mathrm{z}^{[1]}=\left(\begin{array}{c}\mathrm{z}_1^{[1]}\\\mathrm{z}_2^{[1]}\\\mathrm{z}_3^{[1]}\end{array}\right.\right)=\left(\begin{array}{cc}\mathrm{w}_1^{(1]\mathrm{T}}\cdot\mathrm{x}+\mathrm{b}_1^{(1)}\\\mathrm{w}_2^{[1]\mathrm{T}}\cdot\mathrm{x}+\mathrm{b}_2^{[1]}\\\mathrm{w}_3^{[1]\mathrm{T}}\cdot\mathrm{x}+\mathrm{b}_3^{[1]}\end{array}\right)=\left(\begin{array}{c}\mathrm{w}_1^{[1]\mathrm{T}}\cdot\mathrm{x}\\\mathrm{w}_2^{[1]\mathrm{T}}\cdot\mathrm{x}\\\mathrm{w}_3^{(1]\mathrm{T}}\cdot\mathrm{x}\end{array}\right)+\mathrm{b}^{[1]}=\mathrm{W}^{[1]}\mathrm{x}+\mathrm{b}^{[1]}$$
$$\left.\mathrm a^{[1]}=\left(\begin{array}{c}\mathrm a_1^{[1]}\\\mathrm a_2^{[1]}\\\mathrm a_3^{[1]}\end{array}\right.\right)=\left(\begin{array}{c}\mathrm t\left(\mathrm z_1^{(1]}\right)\\\mathrm t\left(\mathrm z_2^{(1]}\right)\\\mathrm t\left(\mathrm z_3^{[1]}\right)\end{array}\right)=\mathrm t\left(\begin{array}{c}\mathrm z_1^{[1]}\\\mathrm z_2^{[1]}\\\mathrm z_3^{[1]}\end{array}\right)=\mathrm t\left(\mathrm z^{[1]}\right)$$

- 上角标中括号用于区分不同层
- 下角标数字表示神经元节点的映射关系
- 一个神经元节点包含上一层节点数$\omega_x$和$b_x$和下一层节点数$Z_y$

### 反向传播
- 梯度下降法
$$\begin{aligned}\boldsymbol{W}&=\boldsymbol{W}-\alpha\frac{\partial\mathrm{L}}{\partial\boldsymbol{W}}\\\mathrm{b}&=\mathrm{b}-\alpha\frac{\partial\mathrm{L}}{\partial\mathrm{b}}\end{aligned}$$
- 向量表达式
$$\left.W^{[1]}=\left(w_1^{[1]},w_2^{[1]},w_3^{[1]}\right)^\mathrm{T}=\left[\begin{array}{c}w_1^{[1]^\mathrm{T}}\\w_2^{[1]^\mathrm{T}}\\w_3^{[1]^\mathrm{T}}\end{array}\right.\right]=\left[\begin{array}{c}\mathrm{w}_{11}^{[1]},\mathrm{w}_{12}^{[1]}\\\mathrm{w}_{21}^{[1]},\mathrm{w}_{22}^{[1]}\\\mathrm{w}_{31}^{[1]},\mathrm{w}_{32}^{[1]}\end{array}\right]\quad\mathrm{b}^{[1]}=\left[\begin{array}{c}\mathrm{b}_1^{[1]}\\\mathrm{b}_2^{[1]}\\\mathrm{b}_3^{[1]}\end{array}\right]$$

## 深层神经网络
随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是"边缘”的特征，第二个隐藏层学习到的是由‘边缘"组成的"形状”的特征，第三个隐藏层学习到的是由"形状"组成的“图案”的特征，最后的隐藏层学习到的是由“图案"组成的"目标"的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。
![[Pasted image 20240102132256.png]]


![[Pasted image 20240102132310.png]]
全连接神经网络可以用来解决回归任务、预测任务和分类任务，在不考虑计算机性能的条件下，无脑设置更深层次的网络模型往往可以取得更好的效果。本质上它是一种线性神经网络，无法避免地要面临处理非线性数据集精度差的问题。优化主要集中在以下几个方面。
非线性因素：围绕激活函数展开来说，提高计算速率就要使激活函数去积分化、去微分化、易求偏导，解决梯度消失和梯度爆炸的问题。
迭代更新策略：围绕反向传播更新权值和偏置，如损失函数选择、优化器选择、学习率衰减策略等等，在一定程度上可以提高精度。这类问题本质上仍是一种寻优算法的探索，可以引入遗传算法、差分进化、多目标优化等寻找pareto最优解，
骨干网络：网络应该设置多少层，每一层应该有多少个节点，从来没有一套标准的设计模板，毫无方向的在不断测试中摸索前进。

# Optimizer
## 批量梯度下降
标准的梯度下降，即批量梯度下降 (batch gradient descent, BGD), 在整个训练集上计算损失函数关于参数 $\theta$ 的梯度：
$$
\theta=\theta-\eta\nabla_\theta J(\theta)
$$
其中 $\theta$ 是模型的参数，$\eta$ 是学习率，$\nabla_\theta J(\theta)$ 为损失函数对参数 $\theta$ 的导数。由于为了一次参数更新我们需要在整个训练集上计算梯度，导致 BGD 可能会非常慢，而且在训练集太大而不能全部载入内存的时候会很棘手。BGD 也不允许我们在线更新模型参数，即实时增加新的训练样本。
BGD 对于凸误差曲面 (convex error surface) 保证收敛到全局最优点，而对于非凸曲面 (non-convex surface) 则是局部最优点。


## 随机梯度下降SGD(Stochastic Gradient Descent)
随机梯度下降（stochastic gradient descent, SGD）则是每次使用一个训练样本$x_i$和标签$y_i$进行一次参数更新：
$$\theta=\theta-\eta\nabla_\theta J(\theta;x^i;y^i)$$
其中$\theta$ 是模型的参数，$\eta$ 是学习率，$\nabla_\theta J(\theta)$ 为损失函数对参数 $\theta$ 的导数。BGD 对于大数据集来说执行了很多冗余的计算，因为在每一次参数更新前都要计算很多相似样本的梯度。SGD 通过一次执行一次更新解决了这种冗余。因此通常 SGD 的速度会非常快而且可以被用于在线学习。SGD以高方差的特点进行连续参数更新，导致目标函数严重震荡。![[Pasted image 20240102161711.png]]

## 小批量梯度下降
小批量梯度下降 (mini-batch gradient descent, MBGD) 则是在上面两种方法中采取了一个折中的办法：每次从训练集中取出 batchsize 个样本作为一个 mini-batch, 以此来进行一次参数更新：

$$
\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i:i+n)};y^{(i:i+n)})
$$

其中 $\theta$ 是模型的参数，$\eta$ 是学习率，$\nabla_\theta J(\theta;x^{(i:i+n)};y^{(i:i+n)})$为损失函数对参数 $\theta$ 的导数，$n$ 为 mini-bach 的大小(batch size)。batch size 越大，批次越少，训练时间会更快一点，但可能造成数据的很大浪费；而batch size 越小，对数据的利用越充分，浪费的数据量越少，但批次会很大，训练会更耗时。

减小参数更新的方差，这样可以有更稳定的收敛。利用现在最先进的深度学习库对矩阵运算进行了高度优化的特点，这样可以使得计算 mini-batch 的梯度更高效。样本数目较大的话，一般的 mini-batch 大小为64到512，考虑到电脑内存设置和使用的方式，如果mini-batch大小是2的n次方，代码会运行地快一些，64就是2的6次方，以此类推，128是2的7次方，256是2的8次方，
512是2的9次方。所以我经常把 mini-batch 大小设成2的次方。

## ADAM(Adaptive Moment Estimation)
Adam是结合了Momentum和Adagrad优点的optimizer，从其迭代公式可以看出，其公式如下：



由上式可以看出，一阶矩估计采用的是Momentum的计算法则，二阶矩估计采用的是Adagrad的计算法则，同时，还针对两种指数加权移动平均进行了偏差校正，防止拟合的初始位置与实际值产生较大偏差。

Adam具有Momentum与Adagrad二者的优点，计算简单，节省内存，同时超参具有可解释性，同时可以进行学习率退火，目标参数不稳定时也可以使用，适用于大规模的数据以及参数的场景，因此被学术界大范围使用，也进化出了诸如AdamW之类的Adam变体等optimizers。

# 卷积神经网络CNN(Convolutional Networks)
## Convolution Operation
对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。

非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。

![[Pasted image 20240102165404.png]]
OK，举个具体的例子。比如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据。
![[Pasted image 20240102165437.png]]
![[Pasted image 20240102170031.png]]
对应位置上是数字先相乘后相加
![[Pasted image 20240102170054.png]]
![[Pasted image 20240102170041.png]]
中间滤波器filter与数据窗口做内积，其具体计算过程则是：$4*0 + 0*0 + 0*0 + 0*0 + 0*1 + 0*1 + 0*0 + 0*1 + -4*2 = -8$

在下图对应的计算过程中，输入是一定区域大小(width*height)的数据，和滤波器filter（带着一组固定权重的神经元）做内积后等到新的二维数据。

具体来说，左边是图像输入，中间部分就是滤波器filter（带着一组固定权重的神经元），不同的滤波器filter会得到不同的输出数据，比如颜色深浅、轮廓。相当于如果想提取图像的不同特征，则用不同的滤波器filter，提取想要的关于图像的特定信息：颜色深浅或轮廓。

如下图所示

![[Pasted image 20240102170214.png]]
在CNN中，滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据。这个过程中，有这么几个参数： 
a. 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。
b. 步长stride：决定滑动多少步可以到边缘。
c. 填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。 
![[Pasted image 20240102170303.png]]
![[Pasted image 20240102170322.png]]

可以看到：
两个神经元，即depth=2，意味着有两个滤波器。
数据窗口每次移动两个步长取3*3的局部数据，即stride=2。zero-padding=1。
然后分别以两个滤波器filter为轴滑动数组进行卷积计算，得到两组不同的结果。

如果初看上图，可能不一定能立马理解啥意思，但结合上文的内容后，理解这个动图已经不是很困难的事情：

左边是输入（7*7*3中，7*7代表图像的像素/长宽，3代表R、G、B 三个颜色通道）
中间部分是两个不同的滤波器Filter w0、Filter w1
最右边则是两个不同的输出
随着左边数据窗口的平移滑动，滤波器Filter w0 / Filter w1对不同的局部数据进行卷积计算。

值得一提的是：左边数据在变化，每次滤波器都是针对某一局部的数据窗口进行卷积，这就是所谓的CNN中的局部感知机制。

打个比方，滤波器就像一双眼睛，人类视角有限，一眼望去，只能看到这世界的局部。如果一眼就看到全世界，你会累死，而且一下子接受全世界所有信息，你大脑接收不过来。当然，即便是看局部，针对局部里的信息人类双眼也是有偏重、偏好的。比如看美女，对脸、胸、腿是重点关注，所以这3个输入的权重相对较大。
与此同时，数据窗口滑动，导致输入在变化，但中间滤波器Filter w0的权重（即每个神经元连接数据窗口的权重）是固定不变的，这个权重不变即所谓的CNN中的参数（权重）共享机制。

再打个比方，某人环游全世界，所看到的信息在变，但采集信息的双眼不变。btw，不同人的双眼 看同一个局部信息 所感受到的不同，即一千个读者有一千个哈姆雷特，所以不同的滤波器 就像不同的双眼，不同的人有着不同的反馈结果。
我第一次看到上面这个动态图的时候，只觉得很炫，另外就是据说计算过程是“相乘后相加”，但到底具体是个怎么相乘后相加的计算过程 则无法一眼看出，网上也没有一目了然的计算过程。本文来细究下。
首先，我们来分解下上述动图，如下图
## ReLU Activation Function
2.2节介绍了激活函数sigmoid，但实际梯度下降中，sigmoid容易饱和、造成终止梯度传递，且没有0中心化。咋办呢，可以尝试另外一个激活函数：ReLU，其图形表示如下
![[Pasted image 20240102171402.png]]
ReLU的优点是收敛快，求梯度简单。
## Padding
![[Pasted image 20240102171518.png]]
![[Pasted image 20240102171536.png]]
As padding size increases, convolution gets more information, especially in the edge.

The attributes of padding：
1. Enable the acquisition of more information.
2. Increase the resolution of the output
## Stride
步长
The attributes of stride:
1. Allow the compression of partial information.
2. Reduce the resolution of the output.
## Pooling
  前头说了，池化，简言之，即取区域平均或最大，如下图所示（图引自cs231n）
  ![[Pasted image 20240102171441.png]]
 上图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4。很简单不是？

![[Pasted image 20240102171758.png]]


## LeNet
![[Pasted image 20240102171930.png]]

## AlexNet
![[Pasted image 20240102171956.png]]

![[Pasted image 20240102172015.png]]


# Word Embedding
![[Pasted image 20240102183707.png]]
1-of-N encoding（one-hot encoding）：每一个词汇都当做一个符号，都用向量来描述，这个方法是有不足的，这样词汇和词汇之间的相关性反映不出来，而且过于稀疏。

Word Class：建立word class，把有相同性质的word放在同一个 class内，将词汇进行分类，这个方法也比较粗糙，比如动物也分了很多种，不能完全概况。

Word Embedding：每一个的词汇也用向量来描述，但是每一个维度是一个属性。word embedding，就是找到一个映射或者函数，生成在一个新的空间上的表达。通俗的翻译可以认为是单词嵌入，就是把X所属空间的单词映射为到Y空间的多维向量，那么该多维向量相当于嵌入到Y所属空间中。与one-hot编码和word class相比，词嵌入可以将更多的信息塞入更低的维度中。

Word Embedding是一种无监督学习方法，通过让模型阅读大量词汇，就可以知道这个embedding的feature长什么样子：

![[Pasted image 20240102184046.png]]
就是找一个神经网络，输入是一个词汇，输出该词对应的word embedding 的 vector。因为输入就是一堆语言的训练集，但是我们没有标签，不知道embedding长什么样。所以说是无监督的。
![[Pasted image 20240102184113.png]]
这里可以用自编码器吗？不行。如果用独热码当作输入，没办法输出任何东西，因为独热码本身并没有什么含义。
## One-hot vector
![[Pasted image 20240102184807.png]]
![[Pasted image 20240102184823.png]]
## Distributional representation
![[Pasted image 20240102184900.png]]
![[Pasted image 20240102184917.png]]
![[Pasted image 20240102185000.png]]
### Continuous bag-of-words model(CBOW model)
![[Pasted image 20240102185101.png]]
![[Pasted image 20240102185133.png]]
![[Pasted image 20240102185144.png]]
![[Pasted image 20240102185155.png]]
![[Pasted image 20240102185204.png]]
![[Pasted image 20240102185214.png]]
![[Pasted image 20240102185228.png]]
![[Pasted image 20240102185244.png]]
### Skip-gram model
![[Pasted image 20240102185115.png]]
![[Pasted image 20240102185601.png]]
![[Pasted image 20240102185616.png]]

![[Pasted image 20240102185624.png]]
![[Pasted image 20240102185633.png]]
![[Pasted image 20240102185640.png]]
![[Pasted image 20240102185648.png]]
# 循环神经网络RNN(Recurrent Neural Network)
  前面我们说了RNN具有时间"记忆"的功能, 那么它是怎么实现所谓的"记忆"的呢?
  ![[Pasted image 20240102191111.png]]
   如图1所示, 我们可以看到RNN层级结构较之于CNN来说比较简单, 它主要有输入层,Hidden Layer, 输出层组成.

并且会发现在Hidden Layer 有一个箭头表示数据的循环更新, 这个就是实现时间记忆功能的方法.

如果到这里你还是没有搞懂RNN到底是什么意思,那么请继续往下看!
![[Pasted image 20240102191342.png]]
 如图2所示为Hidden Layer的层级展开图. t-1, t, t+1表示时间序列. X表示输入的样本. $S_t$表示样本在时间t处的记忆,$S_t = f(W*S_{t-1} +U*X_t)$. W表示输入的权重, U表示此刻输入的样本的权重, V表示输出的样本权重.
在t =1时刻, 一般初始化输入S0=0, 随机初始化W,U,V, 进行下面的公式计算:
![[Pasted image 20240102191550.png]]
其中,f和g均为激活函数. 其中f可以是tanh,relu,sigmoid等激活函数，g通常是softmax也可以是其他。

时间就向前推进，此时的状态s1作为时刻1的记忆状态将参与下一个时刻的预测活动，也就是:
![[Pasted image 20240102191605.png]]
以此类推, 可以得到最终的输出值为:
![[Pasted image 20240102191616.png]]

**注意**: 
1. 这里的**W,U,V**在每个时刻都是相等的(**权重共享**).
***2. 隐藏状态可以理解为:  S=f(现有的输入+过去记忆总结)***

## 反向传播

前面我们介绍了RNN的算法, 它处理时间序列的问题的效果很好, 但是仍然存在着一些问题, 其中较为严重的是容易出现梯度消失或者梯度爆炸的问题(BP算法和长时间依赖造成的). 注意: 这里的梯度消失和BP的不一样,这里主要指由于时间过长而造成记忆值较小的现象.

因此, 就出现了一系列的改进的算法, 这里介绍主要的两种算法: LSTM 和 GRU.

LSTM 和 GRU对于梯度消失或者梯度爆炸的问题处理方法主要是:

对于梯度消失: 由于它们都有特殊的方式存储”记忆”，那么以前梯度比较大的”记忆”不会像简单的RNN一样马上被抹除，因此可以一定程度上克服梯度消失问题。

对于梯度爆炸:用来克服梯度爆炸的问题就是gradient clipping，也就是当你计算的梯度超过阈值c或者小于阈值-c的时候，便把此时的梯度设置成c或-c。 

![[Pasted image 20240102192737.png]]
![[Pasted image 20240102192749.png]]
![[Pasted image 20240102193059.png]]
![[Pasted image 20240102193112.png]]
![[Pasted image 20240102193122.png]]
![[Pasted image 20240102193134.png]]
## LSTM(Long Short Term Memory)算法
![[Pasted image 20240102192409.png]]
![[Pasted image 20240102193150.png]]
 和RNN不同的是: RNN中![[Pasted image 20240102192453.png]],就是个简单的线性求和的过程. 而LSTM可以通过“门”结构来去除或者增加“细胞状态”的信息,实现了对重要内容的保留和对不重要内容的去除. 通过Sigmoid层输出一个0到1之间的概率值，描述每个部分有多少量可以通过，0表示“不允许任务变量通过”，1表示“运行所有变量通过 ”.

用于遗忘的门叫做"遗忘门", 用于信息增加的叫做"信息增加门",最后是用于输出的"输出门". 这里就不展开介绍了.


举个例子，当你想在网上购买生活用品时，一般都会查看一下此前已购买该商品用户的评价。当你浏览评论时，你的大脑下意识地只会记住重要的关键词，比如“amazing”和“awsome”这样的词汇，而不太会关心“this”、“give”、“all”、“should”等字样。如果朋友第二天问你用户评价都说了什么，那你可能不会一字不漏地记住它，而是会说出但大脑里记得的主要观点，比如“下次肯定还会来买”，那其他一些无关紧要的内容自然会从记忆中逐渐消失。
而这基本上就像是 LSTM 或 GRU 所做的那样，它们可以学习只保留相关信息来进行预测，并忘记不相关的数据。简单说，因记忆能力有限，记住重要的，忘记无关紧要的。


LSTM由Hochreiter&Schmidhuber(1997)提出，并在近期被AlexGraves进行了改良和推广。在很多问题，LSTM都取得相当巨大的成功，并得到了广泛的使用。
LSTM通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是LSTM的默认行为，而非需要付出很大代价才能获得的能力！

所有RNN都具有一种重复神经网络模块的链式的形式。在标准的RNN中，这个重复的模块只有一个非常简单的结构，例如一个tanh层。
![[Pasted image 20240102193836.png]]
激活函数 Tanh 作用在于帮助调节流经网络的值，使得数值始终限制在 -1 和 1 之间。
LSTM同样是这样的结构，但是重复的模块拥有一个不同的结构。具体来说，RNN是重复单一的神经网络层，LSTM中的重复模块则包含四个交互的层，三个Sigmoid 和一个tanh层，并以一种非常特殊的方式进行交互。
![[Pasted image 20240102193902.png]]
上图中，σ表示的Sigmoid 激活函数与 tanh 函数类似，不同之处在于 sigmoid 是把值压缩到0~1 之间而不是 -1~1 之间。这样的设置有助于更新或忘记信息：
因为任何数乘以 0 都得 0，这部分信息就会剔除掉；
同样的，任何数乘以 1 都得到它本身，这部分信息就会完美地保存下来
相当于要么是1则记住，要么是0则忘掉，所以还是这个原则：**因记忆能力有限，记住重要的，忘记无关紧要的**。
此外，对于图中使用的各种元素的图标中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表pointwise的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。
![[Pasted image 20240102194018.png]]

LSTM的关键就是细胞状态，水平线在图上方贯穿运行。  
细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。
![[Pasted image 20240102194051.png]]
LSTM有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个sigmoid神经网络层和一个pointwise乘法的非线性操作。
如此，0代表“不许任何量通过”，1就指“允许任意量通过”！从而使得网络就能了解哪些数据是需要遗忘，哪些数据是需要保存。
![[Pasted image 20240102194139.png]]
LSTM拥有三种类型的门结构：遗忘门/忘记门、输入门和输出门，来保护和控制细胞状态。下面，我们来介绍这三个门。

3.1 忘记门
在我们LSTM中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为“忘记门”的结构完成。该忘记门会读取上一个输出和当前输入，做一个Sigmoid 的非线性映射，然后输出一个向量（该向量每一个维度的值都在0到1之间，1表示完全保留，0表示完全舍弃，相当于记住了重要的，忘记了无关紧要的），最后与细胞状态相乘。

类比到语言模型的例子中，则是基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语，进而决定丢弃信息。
![[Pasted image 20240102194222.png]]


大部分初学的读者看到这，可能会有所懵逼，没关系，我们分以下两个步骤理解：

对于上图右侧公式中的权值，准确的说其实是不共享，即是不一样的。有的同学可能第一反应是what？别急，我展开下你可能就瞬间清晰了，即：。
至于右侧公式和左侧的图是怎样的一个一一对应关系呢？如果是用有方向的水流表示计算过程则将一目了然，上动图！红圈表示Sigmoid 激活函数，篮圈表示tanh 函数：
3.2 输入门
下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分：
第一，sigmoid层称“输入门层”决定什么值我们将要更新；
第二，一个tanh层创建一个新的候选值向量，会被加入到状态中。
下一步，我们会讲这两个信息来产生对状态的更新。

在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语，进而确定更新的信息。


![[Pasted image 20240102194244.png]]
继续分两个步骤来理解：

首先，为便于理解图中右侧的两个公式，我们展开下计算过程，即、
其次，上动图！
3.3 细胞状态
现在是更新旧细胞状态的时间了，更新为。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。
我们把旧状态与相乘，丢弃掉我们确定需要丢弃的信息，接着加上。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。
在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方，类似更新细胞状态。
![[Pasted image 20240102194259.png]]


再次动图！

3.4 输出门
最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。

首先，我们运行一个sigmoid层来确定细胞状态的哪个部分将输出出去。
接着，我们把细胞状态通过tanh进行处理（得到一个在-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。

在语言模型的例子中，因为他就看到了一个代词，可能需要输出与一个动词相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化，进而输出信息。

![[Pasted image 20240102194313.png]]

依然分两个步骤来理解：

展开图中右侧第一个公式，
最后一个动图：


![[Pasted image 20240102195122.png]]


# Attention Mechanism
![[Pasted image 20240102195736.png]]

![[Pasted image 20240102195856.png]]

我们可以这样来看待Attention机制：将Source中的构成元素想象成是由一系列的<Key,Value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：
![[Pasted image 20240102200414.png]]
其中$L_x=||Source||$代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。

当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。

从上图可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft-Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。

至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如下图展示的三个阶段。
![[Pasted image 20240102200437.png]]
在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个$Key_i$​，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式
![[Pasted image 20240102200707.png]]
第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似softMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过softMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：
![[Pasted image 20240102200723.png]]
![[Pasted image 20240102200740.png]]
通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。

说完了attention的机制，现在还有一个最重要的问题没有解决，那就是并行计算的问题，解决这个问题就要提到self-attention了。

Self Attention也经常被称为intra-Attention（内部Attention），最近几年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self-Attention模型。

在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self-Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。

如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self-Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self-Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，下图是可视化地表示Self-Attention在同一个英语句子内单词间产生的联系。
![[Pasted image 20240102200755.png]]
![[Pasted image 20240102200800.png]]
从两张图可以看出，Self-Attention可以捕获同一个句子中单词之间的一些句法特征（比如第一张图展示的有一定距离的短语结构）或者语义特征（比如第二张图展示的its的指代对象Law）。

很明显，引入Self-Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。

但是Self-Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self-Attention对于增加计算的并行性也有直接帮助作用。这是为何Self-Attention逐渐被广泛使用的主要原因。到此为止，关于RNN的三个问题就都被解决了。
![[Pasted image 20240102201724.png]]
![[Pasted image 20240102201819.png]]



# Summary
![[Pasted image 20240102221108.png]]![[Pasted image 20240102221142.png]]![[Pasted image 20240102221159.png]]![[Pasted image 20240102221213.png]]![[Pasted image 20240102221220.png]]![[Pasted image 20240102221240.png]]![[Pasted image 20240102221249.png]]![[Pasted image 20240102221258.png]]![[Pasted image 20240102221305.png]]![[Pasted image 20240102221313.png]]![[Pasted image 20240102221320.png]]![[Pasted image 20240102221327.png]]![[Pasted image 20240102221335.png]]![[Pasted image 20240102221343.png]]![[Pasted image 20240102221351.png]]![[Pasted image 20240102221400.png]]![[Pasted image 20240102221408.png]]![[Pasted image 20240102221416.png]]