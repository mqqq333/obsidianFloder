{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pytorch Neural Network Class to Classify MNIST\n",
    "\n",
    "##  Dataset Introduction\n",
    "MNIST includes 60,000 28x28 training samples and 10,000 test samples.Many tutorials will start with it, as the \"Hello World\" problem in Machine Learning.So here we will also use MNIST for practice.\n",
    "\n",
    "And we will also build a convolutional neural network from scratch, which can achieve an accuracy of 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512 #batch size = the number of training examples in one forward/backward pass\n",
    "EPOCHS = 10 #an epoch in a neural network is the training of the neural network with all the training data for one cycle\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Let torch determine whether to use the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Pytorch contains the MNIST data set, we can use it directly here.\n",
    "\n",
    "If it is executed for the first time, the data folder will be generated and it will take some time to download. If it has been downloaded before, it will not be downloaded again.\n",
    "\n",
    "Since the official has implemented the dataset, DataLoader can be used directly to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))# https://medium.com/nerd-for-tech/overview-of-normalization-techniques-in-deep-learning-e12a79060daf\n",
    "                       ])),# more details at https://pytorch.org/vision/stable/transforms.html\n",
    "        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:torch.Size([512, 1, 28, 28]), label shape:torch.Size([512])\n",
      "label:6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1670b3f7400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(train_loader))\n",
    "print(f\"image shape:{image.shape}, label shape:{label.shape}\")\n",
    "print(f\"label:{label[0]}\")\n",
    "plt.imshow(image[0][0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a neural network.The network contains two convolutional layers, conv1 and conv2, followed by two linear layers as output, and finally outputs 10 dimensions. We use these 10 dimensions as the identifier of 0-9 to determine which number is recognized.\n",
    "\n",
    "The input and output dimensions of each layer have been marked here as comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1,28x28\n",
    "        self.conv1 = nn.Conv2d(1,10,5) # 10, 24x24\n",
    "        self.conv2 = nn.Conv2d(10,20,3) # 128, 10x10\n",
    "        self.fc1 = nn.Linear(20*10*10,500)\n",
    "        self.fc2 = nn.Linear(500,10)\n",
    "    def forward(self,x):\n",
    "        #x (batch_size(512), channels(1), height(28), width(28))\n",
    "        batch_size = x.size(0)\n",
    "        out = self.conv1(x) #512x10x24x24\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, 2)  #512x20x12x12\n",
    "        out = self.conv2(out) #512x20x10x10\n",
    "        out = F.relu(out)\n",
    "        out = out.view(batch_size,-1)#512x2000\n",
    "        out = self.fc1(out)#512x500\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)#512x10\n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a network and use the .to method to move the network to the GPU after instantiation.\n",
    "\n",
    "We also directly choose Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters()) #more details about Adam at https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next define the training function. We encapsulate all training operations into this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch): # it's equal to the method \"fit\" of scikit-learn\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()#why need to call zero_grad():https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        output = model(data)#the output is the prediction\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()#equal to the backpropagation\n",
    "        optimizer.step()#update the parameters\n",
    "        if(batch_idx+1)%30 == 0: \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test operation is also encapsulated into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # Add up the losses of a batch\n",
    "            pred = output.max(1, keepdim=True)[1] # Find the index with the greatest probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the training. The benefits of encapsulation are reflected here. Just write two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.430581\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.252417\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.159174\n",
      "\n",
      "Test set: Average loss: 0.0863, Accuracy: 9749/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.086607\n",
      "Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.086694\n",
      "Train Epoch: 2 [45568/60000 (75%)]\tLoss: 0.071649\n",
      "\n",
      "Test set: Average loss: 0.0552, Accuracy: 9828/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [14848/60000 (25%)]\tLoss: 0.051280\n",
      "Train Epoch: 3 [30208/60000 (50%)]\tLoss: 0.047238\n",
      "Train Epoch: 3 [45568/60000 (75%)]\tLoss: 0.056056\n",
      "\n",
      "Test set: Average loss: 0.0538, Accuracy: 9828/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [14848/60000 (25%)]\tLoss: 0.037606\n",
      "Train Epoch: 4 [30208/60000 (50%)]\tLoss: 0.059946\n",
      "Train Epoch: 4 [45568/60000 (75%)]\tLoss: 0.024784\n",
      "\n",
      "Test set: Average loss: 0.0438, Accuracy: 9860/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [14848/60000 (25%)]\tLoss: 0.021791\n",
      "Train Epoch: 5 [30208/60000 (50%)]\tLoss: 0.019826\n",
      "Train Epoch: 5 [45568/60000 (75%)]\tLoss: 0.026461\n",
      "\n",
      "Test set: Average loss: 0.0372, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [14848/60000 (25%)]\tLoss: 0.025186\n",
      "Train Epoch: 6 [30208/60000 (50%)]\tLoss: 0.021943\n",
      "Train Epoch: 6 [45568/60000 (75%)]\tLoss: 0.016271\n",
      "\n",
      "Test set: Average loss: 0.0372, Accuracy: 9880/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [14848/60000 (25%)]\tLoss: 0.031807\n",
      "Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.033691\n",
      "Train Epoch: 7 [45568/60000 (75%)]\tLoss: 0.015460\n",
      "\n",
      "Test set: Average loss: 0.0317, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [14848/60000 (25%)]\tLoss: 0.015948\n",
      "Train Epoch: 8 [30208/60000 (50%)]\tLoss: 0.011563\n",
      "Train Epoch: 8 [45568/60000 (75%)]\tLoss: 0.021807\n",
      "\n",
      "Test set: Average loss: 0.0340, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [14848/60000 (25%)]\tLoss: 0.010993\n",
      "Train Epoch: 9 [30208/60000 (50%)]\tLoss: 0.009591\n",
      "Train Epoch: 9 [45568/60000 (75%)]\tLoss: 0.009621\n",
      "\n",
      "Test set: Average loss: 0.0318, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [14848/60000 (25%)]\tLoss: 0.012057\n",
      "Train Epoch: 10 [30208/60000 (50%)]\tLoss: 0.013126\n",
      "Train Epoch: 10 [45568/60000 (75%)]\tLoss: 0.010834\n",
      "\n",
      "Test set: Average loss: 0.0378, Accuracy: 9887/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the result, the accuracy is 99%, no problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally letâ€™s visualize the correct and incorrect examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the correct case:\n",
      "the label of the data image:8\n",
      "the predict of the data image:8\n",
      "the data image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data, target in test_loader:\n",
    "    data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "    output = model(data)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    correct_index = torch.where(torch.squeeze(pred)==target)\n",
    "    if(len(correct_index[0])!=0):\n",
    "        # the correct case\n",
    "        print(f'the correct case:')\n",
    "        print(f'the label of the data image:{target[correct_index[0][0]]}')\n",
    "        print(f'the predict of the data image:{torch.squeeze(pred[correct_index[0][0]])}')\n",
    "        print(f'the data image:')\n",
    "        plt.imshow(data[correct_index[0][0]].cpu().reshape(28,28), cmap=plt.cm.gray_r)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the incorrect case:\n",
      "the label of the data image:9\n",
      "the predict of the data image:7\n",
      "the data image:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOWklEQVR4nO3df4xV9ZnH8c+zbKtE+APKhEyAWUqjiUSztE5II4Sw1EUxJkPjr/JHgwnZwYixJSQusUZIiIlBC2l0JZkqKRiWWqUG/iBtFQqGmDTeIawOkl0RRwsZYfAHtQao0mf/mEMz4NzvGe65v2ae9yuZ3Dvnud85j1c/nnvP9577NXcXgNHvnxrdAID6IOxAEIQdCIKwA0EQdiCIf67nziZNmuTTp0+v5y6BUHp7e3X69GkbqlYo7GZ2m6RfSBoj6Tl3fyL1+OnTp6tUKhXZJYCE9vb2srWKX8ab2RhJ/yVpkaSZkpaY2cxK/x6A2irynn22pKPufszd/ybp15I6qtMWgGorEvYpkv486Pfj2bZLmFmnmZXMrNTf319gdwCKqPnZeHfvcvd2d29vaWmp9e4AlFEk7CckTRv0+9RsG4AmVCTsb0q61sy+bWbflPQjSbuq0xaAaqt46s3dvzKzByX9XgNTb5vd/XDVOgNQVYXm2d19t6TdVeoFQA3xcVkgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIWWbDazXkmfS7og6St3b69GUwCqr1DYM//m7qer8HcA1BAv44EgiobdJf3BzLrNrHOoB5hZp5mVzKzU399fcHcAKlU07HPd/XuSFklaYWbzLn+Au3e5e7u7t7e0tBTcHYBKFQq7u5/Ibk9JekXS7Go0BaD6Kg67mV1jZuMv3pe0UFJPtRoDUF1FzsZPlvSKmV38O//t7r+rSlcAqq7isLv7MUn/WsVeANQQU29AEIQdCIKwA0EQdiAIwg4EUY0LYTCK7d27N1lfs2ZNsn7gwIGK9z127NhkfePGjcn68uXLK973aMSRHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJ49uJUrVybrzz77bLL+5ZdfVrzv7PLoss6dO5esr1u3Lllnnv1SHNmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjm2UeB7u7usrVly5Ylxx45ciRZb2trS9b7+vqS9S+++CJZT5kwYUKyPmPGjGT9zJkzZWvjxo1Ljh0zZkyyPhJxZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIJhnbwKffvppsv7kk08m688991zZWn9/f3LsxIkTk/U5c+Yk62fPnk3WFyxYULaWN08+c+bMZP2ZZ55J1q+//vqytSVLliTHPvroo8l63mcAmlHukd3MNpvZKTPrGbRtopm9ambvZrcj758cCGY4L+N/Jem2y7atlrTH3a+VtCf7HUATyw27u78u6ZPLNndI2pLd3yJpcXXbAlBtlZ6gm+zuFz8U/ZGkyeUeaGadZlYys1Le+0cAtVP4bLy7uyRP1Lvcvd3d21taWoruDkCFKg37STNrlaTs9lT1WgJQC5WGfZekpdn9pZJ2VqcdALWSO89uZtslzZc0ycyOS1oj6QlJvzGzZZI+kHRPLZsc6dauXZusr1+/PlnPm8tOufHGG5P1xx57LFm/6667Kt53UefPn0/W33vvvWQ9da39hg0bkmOXLl2arI/EefbcsLt7uU8f/KDKvQCoIT4uCwRB2IEgCDsQBGEHgiDsQBBc4jpM+/btK1vbtm1bcuzHH3+crD/11FPJ+lVXXZWsf/bZZ2VrDzzwQHLs2LFjk/VayptSfPzxx5P1l19+ueJ9T506NVlvbW2t+G83K47sQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE8+yZDz/8MFk/fvx42dodd9yRHNvR0VFRT6Pd/v37k/VNmzYV+vtXX3112dqOHTuSY0fjtypxZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIJhnz5w7dy5ZT82Vb9++PTm2q6srWe/s7EzWizhw4ECynrdkc96yya+99lqyfsstt5St7d27Nzk2bynrPPfdd1/Z2uzZswv97ZGIIzsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME8eybvO8yffvrpsrV58+Ylx86dO7einobr2LFjFY9NXfMtSd3d3cn67t27k/XU0sZ5c/Q33XRTsp5aklkqfj38aJN7ZDezzWZ2ysx6Bm1ba2YnzOxQ9nN7bdsEUNRwXsb/StJtQ2zf6O6zsp/0/94BNFxu2N39dUmf1KEXADVU5ATdg2b2VvYyv+wbMzPrNLOSmZX6+/sL7A5AEZWGfZOk70iaJalP0s/LPdDdu9y93d3bR+OX+AEjRUVhd/eT7n7B3f8u6ZeS4l1CBIwwFYXdzAavZ/tDST3lHgugOeTOs5vZdknzJU0ys+OS1kiab2azJLmkXknLa9difeRdt516C/Liiy8mx+adq8j73vnUHL+U7r2trS05dsaMGcl6nrznLbX+e+padyl/3fqdO3cm67hUbtjdfckQm5+vQS8AaoiPywJBEHYgCMIOBEHYgSAIOxAEl7hm8r62+N577y1be//995NjV61alaybWbKeNzXX2tpatjZ+/Pjk2KJSU2uSdP78+bK1Xbt2JcfmXTo8f/78ZB2X4sgOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewz5554403kvUpU6aUreV9ZfENN9xQUU8XXXfddYXG11Le11jffffdZWt5l/5u3bo1Wa/1ZwhGG47sQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE8+yZxYsXF6qPVr29vcn66tWrKx5/9OjR5NjUcs+4chzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAI5tmD6+npSdYfeuihZH3fvn3Jemoennn0+so9spvZNDP7o5m9Y2aHzewn2faJZvaqmb2b3fJvDmhiw3kZ/5WkVe4+U9L3Ja0ws5mSVkva4+7XStqT/Q6gSeWG3d373P1gdv9zSUckTZHUIWlL9rAtkhbXqEcAVXBFJ+jMbLqk70r6k6TJ7t6XlT6SNLnMmE4zK5lZKe87xwDUzrDDbmbjJO2Q9FN3/8vgmru7JB9qnLt3uXu7u7e3tLQUahZA5YYVdjP7hgaCvs3df5ttPmlmrVm9VdKp2rQIoBpyp95sYD3h5yUdcfcNg0q7JC2V9ER2u7MmHaKQM2fOJOsPP/xwsl4qlZL1m2++OVm/8847k3XUz3Dm2edI+rGkt83sULbtEQ2E/DdmtkzSB5LuqUmHAKoiN+zufkCSlSn/oLrtAKgVPi4LBEHYgSAIOxAEYQeCIOxAEFziOsrlXaJ64cKFZH39+vXJ+v3333/FPaExOLIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBDMs48A58+fT9ZT15QfPHgwOXbRokXJOvPoowdHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0Ignn2JnD27NlkfcGCBcl6b29v2dq6deuSY1euXJmsY/TgyA4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQQxnffZpkrZKmizJJXW5+y/MbK2k/5DUnz30EXffXatGR7K869FXrFiRrB8+fDhZf+GFF8rWOjo6kmMRx3A+VPOVpFXuftDMxkvqNrNXs9pGd3+qdu0BqJbhrM/eJ6kvu/+5mR2RNKXWjQGorit6z25m0yV9V9Kfsk0PmtlbZrbZzCaUGdNpZiUzK/X39w/1EAB1MOywm9k4STsk/dTd/yJpk6TvSJqlgSP/z4ca5+5d7t7u7u0tLS3FOwZQkWGF3cy+oYGgb3P330qSu5909wvu/ndJv5Q0u3ZtAigqN+xmZpKel3TE3TcM2t466GE/lNRT/fYAVMtwzsbPkfRjSW+b2aFs2yOSlpjZLA1Mx/VKWl6D/kaF/fv3J+ttbW3J+ksvvZSs33rrrVfcE+IZztn4A5JsiBJz6sAIwifogCAIOxAEYQeCIOxAEIQdCIKwA0HwVdJ1sHDhwkJ1oBo4sgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEObu9duZWb+kDwZtmiTpdN0auDLN2luz9iXRW6Wq2du/uPuQ3/9W17B/bedmJXdvb1gDCc3aW7P2JdFbperVGy/jgSAIOxBEo8Pe1eD9pzRrb83al0RvlapLbw19zw6gfhp9ZAdQJ4QdCKIhYTez28zsf83sqJmtbkQP5ZhZr5m9bWaHzKzU4F42m9kpM+sZtG2imb1qZu9mt0Ousdeg3taa2YnsuTtkZrc3qLdpZvZHM3vHzA6b2U+y7Q197hJ91eV5q/t7djMbI+n/JP27pOOS3pS0xN3fqWsjZZhZr6R2d2/4BzDMbJ6kv0ra6u43ZNvWS/rE3Z/I/kc5wd3/s0l6Wyvpr41exjtbrah18DLjkhZLuk8NfO4Sfd2jOjxvjTiyz5Z01N2PufvfJP1aUkcD+mh67v66pE8u29whaUt2f4sG/mOpuzK9NQV373P3g9n9zyVdXGa8oc9doq+6aETYp0j686Dfj6u51nt3SX8ws24z62x0M0OY7O592f2PJE1uZDNDyF3Gu54uW2a8aZ67SpY/L4oTdF83192/J2mRpBXZy9Wm5APvwZpp7nRYy3jXyxDLjP9DI5+7Spc/L6oRYT8hadqg36dm25qCu5/Ibk9JekXNtxT1yYsr6Ga3pxrczz800zLeQy0zriZ47hq5/Hkjwv6mpGvN7Ntm9k1JP5K0qwF9fI2ZXZOdOJGZXSNpoZpvKepdkpZm95dK2tnAXi7RLMt4l1tmXA1+7hq+/Lm71/1H0u0aOCP/nqSfNaKHMn3NkPQ/2c/hRvcmabsGXtZ9qYFzG8skfUvSHknvSnpN0sQm6u0FSW9LeksDwWptUG9zNfAS/S1Jh7Kf2xv93CX6qsvzxsdlgSA4QQcEQdiBIAg7EARhB4Ig7EAQhB0IgrADQfw/qpJXE8k6okAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data, target in test_loader:\n",
    "    data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "    output = model(data)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    incorrect_index = torch.where(torch.squeeze(pred)!=target)\n",
    "    if(len(incorrect_index[0])!=0):\n",
    "        # the incorrect case\n",
    "        print(f'the incorrect case:')\n",
    "        print(f'the label of the data image:{target[incorrect_index[0][0]]}')\n",
    "        print(f'the predict of the data image:{torch.squeeze(pred[incorrect_index[0][0]])}')\n",
    "        print(f'the data image:')\n",
    "        plt.imshow(data[incorrect_index[0][0]].cpu().reshape(28,28), cmap=plt.cm.gray_r)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a very simple data set. Due to its limitations, it can only be used for research purposes and brings very limited value to practical applications. But through this example, we can fully understand the workflow of an actual project\n",
    "\n",
    "We import the data set, preprocess the data, define our model, adjust the hyperparameters, test the training, and then adjust the hyperparameters or adjust the model through the training results.\n",
    "\n",
    "Through this, we already have a good template. Future projects can use this template as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
